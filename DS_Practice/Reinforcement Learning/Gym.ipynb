{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env class Comes with certain important methods/attributes:\n",
    "\n",
    "-action_space\n",
    "\n",
    "-observation_space\n",
    "\n",
    "-reset(): returns initial state of the game and also resets the enviornment(comes back to starting point)\n",
    "\n",
    "-step()\n",
    "\n",
    "-render(): pop-up window of the game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04289837,  0.02267138, -0.03820491, -0.0255932 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset gave 4 variables that means the enviornment can be defined by 4 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\verma\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.02661521, 0.04775303, 0.00654318, 0.02198249])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(1000): #runs for 1000 steps.\n",
    "    action=env.action_space.sample() #it consists of actions. see below\n",
    "    env.step(action)\n",
    "    env.render()\n",
    "env.close()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 means it has only two actions left and right movement of cartpole. in short it gives how many axtions\n",
    "#you can perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space #used to represent n-dimensional tensor in gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0] #no. of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Playing Games with Random Strategy\n",
    "\n",
    "-Game Episode(Start to end(game over)--->one episode)\n",
    "\n",
    "-Step() function in more detail\n",
    "\n",
    "-Game Over?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will have multiple game episodes.\n",
    "#bENEFIT of this is that in the beiginning of the game\n",
    "#when the agent is new to enviornment that what action needs to be taken. and will learn step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode :0/20 High Score :12\n",
      "Game Episode :1/20 High Score :16\n",
      "Game Episode :2/20 High Score :37\n",
      "Game Episode :3/20 High Score :24\n",
      "Game Episode :4/20 High Score :17\n",
      "Game Episode :5/20 High Score :22\n",
      "Game Episode :6/20 High Score :33\n",
      "Game Episode :7/20 High Score :14\n",
      "Game Episode :8/20 High Score :28\n",
      "Game Episode :9/20 High Score :28\n",
      "Game Episode :10/20 High Score :15\n",
      "Game Episode :11/20 High Score :9\n",
      "Game Episode :12/20 High Score :12\n",
      "Game Episode :13/20 High Score :31\n",
      "Game Episode :14/20 High Score :37\n",
      "Game Episode :15/20 High Score :16\n",
      "Game Episode :16/20 High Score :17\n",
      "Game Episode :17/20 High Score :15\n",
      "Game Episode :18/20 High Score :34\n",
      "Game Episode :19/20 High Score :10\n",
      "All 20 episodes are over\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for e in range(20): #e-->episode\n",
    "    #Play 20 episodes\n",
    "    observation = env.reset()\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation,reward,done,other_info=env.step(action) #step return four things written below\n",
    "        \n",
    "        if done:\n",
    "            #Game episode is over\n",
    "            print(\"Game Episode :{}/{} High Score :{}\".format(e,20,t))\n",
    "            break\n",
    "        \n",
    "env.close()\n",
    "print(\"All 20 episodes are over\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step()\n",
    "\n",
    "-observation\n",
    "\n",
    "-reward\n",
    "\n",
    "-done\n",
    "\n",
    "-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Q-Learning\n",
    "\n",
    "Agent Design and Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size): #in this game only 4-state size and 2-actions\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size \n",
    "        self.memory=deque(maxlen=2000)\n",
    "        self.gamma=0.95 #discount factor in RL\n",
    "        \n",
    "        #Exploration vs Exploitation Tradeoff\n",
    "        \n",
    "        #Exploration: Good in the beginnning--> helps you to try various random things\n",
    "        #Exploitation: Sample good experience from the past(memory)--->good in the end.\n",
    "        \n",
    "        self.epsilon=1.0 #100% random exploration in the beginning.\n",
    "        self.epsilon_decay=0.995\n",
    "        self.epsilon_min=0.01\n",
    "        self.learning_rate=0.001\n",
    "        self.model=self._create_model()\n",
    "    \n",
    "    def _create_model(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(24,input_dim=4,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(2,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.001)) #kind of linear regression\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        #remeber past experience\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "    def act(self,state):\n",
    "        #Sampling according Epsilon greedy method\n",
    "        \n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            #Take a random action\n",
    "            return random.randrange(self.action_size)\n",
    "        #Ask neural network to give me the most suitable action\n",
    "        return np.argmax(self.model.predict(state)[0])#[0] because it gives list of list\n",
    "    \n",
    "    def train(self,batch_size=32):\n",
    "        #Training using a replay buffer\n",
    "        #We have to sample out batch of experiences forom the buffer/memory and feed every expericence one by one to our neural network\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for experience in minibatch:\n",
    "            state,action,reward,next_state,done=experience\n",
    "            \n",
    "            # to train we need , X,y:state,expected reward(reward is the bellman equation that we derived)\n",
    "            if not done:\n",
    "                #game is not yet over, use bellman equation to apprx the target_Value of reward\n",
    "                target=reward+self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target=reward\n",
    "            \n",
    "            target_f=self.model.predict(state) \n",
    "            target_f[0][action]=target\n",
    "            \n",
    "            #X=state,Y=target_f\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon*=self.epsilon_decay \n",
    "            \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training the DQN agent(Deep Q-learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=1000\n",
    "output_dir=\"cartpole_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent(state_size=4,action_size=2)\n",
    "done=False\n",
    "state_size=4\n",
    "action_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode :0/20 High Score :10 Exploration Rate:1.0\n",
      "Game Episode :1/20 High Score :10 Exploration Rate:1.0\n",
      "Game Episode :2/20 High Score :10 Exploration Rate:0.99\n",
      "Game Episode :3/20 High Score :10 Exploration Rate:0.99\n",
      "Game Episode :4/20 High Score :10 Exploration Rate:0.99\n",
      "Game Episode :5/20 High Score :10 Exploration Rate:0.98\n",
      "Game Episode :6/20 High Score :10 Exploration Rate:0.98\n",
      "Game Episode :7/20 High Score :10 Exploration Rate:0.97\n",
      "Game Episode :8/20 High Score :10 Exploration Rate:0.97\n",
      "Game Episode :9/20 High Score :10 Exploration Rate:0.96\n",
      "Game Episode :10/20 High Score :10 Exploration Rate:0.96\n",
      "Game Episode :11/20 High Score :10 Exploration Rate:0.95\n",
      "Game Episode :12/20 High Score :10 Exploration Rate:0.95\n",
      "Game Episode :13/20 High Score :10 Exploration Rate:0.94\n",
      "Game Episode :14/20 High Score :10 Exploration Rate:0.94\n",
      "Game Episode :15/20 High Score :10 Exploration Rate:0.93\n",
      "Game Episode :16/20 High Score :10 Exploration Rate:0.93\n",
      "Game Episode :17/20 High Score :10 Exploration Rate:0.92\n",
      "Game Episode :18/20 High Score :10 Exploration Rate:0.92\n",
      "Game Episode :19/20 High Score :10 Exploration Rate:0.91\n",
      "Game Episode :20/20 High Score :10 Exploration Rate:0.91\n",
      "Game Episode :21/20 High Score :10 Exploration Rate:0.9\n",
      "Game Episode :22/20 High Score :10 Exploration Rate:0.9\n",
      "Game Episode :23/20 High Score :10 Exploration Rate:0.9\n",
      "Game Episode :24/20 High Score :10 Exploration Rate:0.89\n",
      "Game Episode :25/20 High Score :10 Exploration Rate:0.89\n",
      "Game Episode :26/20 High Score :10 Exploration Rate:0.88\n",
      "Game Episode :27/20 High Score :10 Exploration Rate:0.88\n",
      "Game Episode :28/20 High Score :10 Exploration Rate:0.87\n",
      "Game Episode :29/20 High Score :10 Exploration Rate:0.87\n",
      "Game Episode :30/20 High Score :10 Exploration Rate:0.86\n",
      "Game Episode :31/20 High Score :10 Exploration Rate:0.86\n",
      "Game Episode :32/20 High Score :10 Exploration Rate:0.86\n",
      "Game Episode :33/20 High Score :10 Exploration Rate:0.85\n",
      "Game Episode :34/20 High Score :10 Exploration Rate:0.85\n",
      "Game Episode :35/20 High Score :10 Exploration Rate:0.84\n",
      "Game Episode :36/20 High Score :10 Exploration Rate:0.84\n",
      "Game Episode :37/20 High Score :10 Exploration Rate:0.83\n",
      "Game Episode :38/20 High Score :10 Exploration Rate:0.83\n",
      "Game Episode :39/20 High Score :10 Exploration Rate:0.83\n",
      "Game Episode :40/20 High Score :10 Exploration Rate:0.82\n",
      "Game Episode :41/20 High Score :10 Exploration Rate:0.82\n",
      "Game Episode :42/20 High Score :10 Exploration Rate:0.81\n",
      "Game Episode :43/20 High Score :10 Exploration Rate:0.81\n",
      "Game Episode :44/20 High Score :10 Exploration Rate:0.81\n",
      "Game Episode :45/20 High Score :10 Exploration Rate:0.8\n",
      "Game Episode :46/20 High Score :10 Exploration Rate:0.8\n",
      "Game Episode :47/20 High Score :10 Exploration Rate:0.79\n",
      "Game Episode :48/20 High Score :10 Exploration Rate:0.79\n",
      "Game Episode :49/20 High Score :10 Exploration Rate:0.79\n",
      "Game Episode :50/20 High Score :10 Exploration Rate:0.78\n",
      "Game Episode :51/20 High Score :10 Exploration Rate:0.78\n",
      "Game Episode :52/20 High Score :10 Exploration Rate:0.77\n",
      "Game Episode :53/20 High Score :10 Exploration Rate:0.77\n",
      "Game Episode :54/20 High Score :10 Exploration Rate:0.77\n",
      "Game Episode :55/20 High Score :10 Exploration Rate:0.76\n",
      "Game Episode :56/20 High Score :10 Exploration Rate:0.76\n",
      "Game Episode :57/20 High Score :10 Exploration Rate:0.76\n",
      "Game Episode :58/20 High Score :10 Exploration Rate:0.75\n",
      "Game Episode :59/20 High Score :10 Exploration Rate:0.75\n",
      "Game Episode :60/20 High Score :10 Exploration Rate:0.74\n",
      "Game Episode :61/20 High Score :10 Exploration Rate:0.74\n",
      "Game Episode :62/20 High Score :10 Exploration Rate:0.74\n",
      "Game Episode :63/20 High Score :10 Exploration Rate:0.73\n",
      "Game Episode :64/20 High Score :10 Exploration Rate:0.73\n",
      "Game Episode :65/20 High Score :10 Exploration Rate:0.73\n",
      "Game Episode :66/20 High Score :10 Exploration Rate:0.72\n",
      "Game Episode :67/20 High Score :10 Exploration Rate:0.72\n",
      "Game Episode :68/20 High Score :10 Exploration Rate:0.71\n",
      "Game Episode :69/20 High Score :10 Exploration Rate:0.71\n",
      "Game Episode :70/20 High Score :10 Exploration Rate:0.71\n",
      "Game Episode :71/20 High Score :10 Exploration Rate:0.7\n",
      "Game Episode :72/20 High Score :10 Exploration Rate:0.7\n",
      "Game Episode :73/20 High Score :10 Exploration Rate:0.7\n",
      "Game Episode :74/20 High Score :10 Exploration Rate:0.69\n",
      "Game Episode :75/20 High Score :10 Exploration Rate:0.69\n",
      "Game Episode :76/20 High Score :10 Exploration Rate:0.69\n",
      "Game Episode :77/20 High Score :10 Exploration Rate:0.68\n",
      "Game Episode :78/20 High Score :10 Exploration Rate:0.68\n",
      "Game Episode :79/20 High Score :10 Exploration Rate:0.68\n",
      "Game Episode :80/20 High Score :10 Exploration Rate:0.67\n",
      "Game Episode :81/20 High Score :10 Exploration Rate:0.67\n",
      "Game Episode :82/20 High Score :10 Exploration Rate:0.67\n",
      "Game Episode :83/20 High Score :10 Exploration Rate:0.66\n",
      "Game Episode :84/20 High Score :10 Exploration Rate:0.66\n",
      "Game Episode :85/20 High Score :10 Exploration Rate:0.66\n",
      "Game Episode :86/20 High Score :10 Exploration Rate:0.65\n",
      "Game Episode :87/20 High Score :10 Exploration Rate:0.65\n",
      "Game Episode :88/20 High Score :10 Exploration Rate:0.65\n",
      "Game Episode :89/20 High Score :10 Exploration Rate:0.64\n",
      "Game Episode :90/20 High Score :10 Exploration Rate:0.64\n",
      "Game Episode :91/20 High Score :10 Exploration Rate:0.64\n",
      "Game Episode :92/20 High Score :10 Exploration Rate:0.63\n",
      "Game Episode :93/20 High Score :10 Exploration Rate:0.63\n",
      "Game Episode :94/20 High Score :10 Exploration Rate:0.63\n",
      "Game Episode :95/20 High Score :10 Exploration Rate:0.62\n",
      "Game Episode :96/20 High Score :10 Exploration Rate:0.62\n",
      "Game Episode :97/20 High Score :10 Exploration Rate:0.62\n",
      "Game Episode :98/20 High Score :10 Exploration Rate:0.61\n",
      "Game Episode :99/20 High Score :10 Exploration Rate:0.61\n",
      "Game Episode :100/20 High Score :10 Exploration Rate:0.61\n",
      "Game Episode :101/20 High Score :10 Exploration Rate:0.61\n",
      "Game Episode :102/20 High Score :10 Exploration Rate:0.6\n",
      "Game Episode :103/20 High Score :10 Exploration Rate:0.6\n",
      "Game Episode :104/20 High Score :10 Exploration Rate:0.6\n",
      "Game Episode :105/20 High Score :10 Exploration Rate:0.59\n",
      "Game Episode :106/20 High Score :10 Exploration Rate:0.59\n",
      "Game Episode :107/20 High Score :10 Exploration Rate:0.59\n",
      "Game Episode :108/20 High Score :10 Exploration Rate:0.58\n",
      "Game Episode :109/20 High Score :10 Exploration Rate:0.58\n",
      "Game Episode :110/20 High Score :10 Exploration Rate:0.58\n",
      "Game Episode :111/20 High Score :10 Exploration Rate:0.58\n",
      "Game Episode :112/20 High Score :10 Exploration Rate:0.57\n",
      "Game Episode :113/20 High Score :10 Exploration Rate:0.57\n",
      "Game Episode :114/20 High Score :10 Exploration Rate:0.57\n",
      "Game Episode :115/20 High Score :10 Exploration Rate:0.56\n",
      "Game Episode :116/20 High Score :10 Exploration Rate:0.56\n",
      "Game Episode :117/20 High Score :10 Exploration Rate:0.56\n",
      "Game Episode :118/20 High Score :10 Exploration Rate:0.56\n",
      "Game Episode :119/20 High Score :10 Exploration Rate:0.55\n",
      "Game Episode :120/20 High Score :10 Exploration Rate:0.55\n",
      "Game Episode :121/20 High Score :10 Exploration Rate:0.55\n",
      "Game Episode :122/20 High Score :10 Exploration Rate:0.55\n",
      "Game Episode :123/20 High Score :10 Exploration Rate:0.54\n",
      "Game Episode :124/20 High Score :10 Exploration Rate:0.54\n",
      "Game Episode :125/20 High Score :10 Exploration Rate:0.54\n",
      "Game Episode :126/20 High Score :10 Exploration Rate:0.53\n",
      "Game Episode :127/20 High Score :10 Exploration Rate:0.53\n",
      "Game Episode :128/20 High Score :10 Exploration Rate:0.53\n",
      "Game Episode :129/20 High Score :10 Exploration Rate:0.53\n",
      "Game Episode :130/20 High Score :10 Exploration Rate:0.52\n",
      "Game Episode :131/20 High Score :10 Exploration Rate:0.52\n",
      "Game Episode :132/20 High Score :10 Exploration Rate:0.52\n",
      "Game Episode :133/20 High Score :10 Exploration Rate:0.52\n",
      "Game Episode :134/20 High Score :10 Exploration Rate:0.51\n",
      "Game Episode :135/20 High Score :10 Exploration Rate:0.51\n",
      "Game Episode :136/20 High Score :10 Exploration Rate:0.51\n",
      "Game Episode :137/20 High Score :10 Exploration Rate:0.51\n",
      "Game Episode :138/20 High Score :10 Exploration Rate:0.5\n",
      "Game Episode :139/20 High Score :10 Exploration Rate:0.5\n",
      "Game Episode :140/20 High Score :10 Exploration Rate:0.5\n",
      "Game Episode :141/20 High Score :10 Exploration Rate:0.5\n",
      "Game Episode :142/20 High Score :10 Exploration Rate:0.49\n",
      "Game Episode :143/20 High Score :10 Exploration Rate:0.49\n",
      "Game Episode :144/20 High Score :10 Exploration Rate:0.49\n",
      "Game Episode :145/20 High Score :10 Exploration Rate:0.49\n",
      "Game Episode :146/20 High Score :10 Exploration Rate:0.48\n",
      "Game Episode :147/20 High Score :10 Exploration Rate:0.48\n",
      "Game Episode :148/20 High Score :10 Exploration Rate:0.48\n",
      "Game Episode :149/20 High Score :10 Exploration Rate:0.48\n",
      "Game Episode :150/20 High Score :10 Exploration Rate:0.47\n",
      "Game Episode :151/20 High Score :10 Exploration Rate:0.47\n",
      "Game Episode :152/20 High Score :10 Exploration Rate:0.47\n",
      "Game Episode :153/20 High Score :10 Exploration Rate:0.47\n",
      "Game Episode :154/20 High Score :10 Exploration Rate:0.46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-ba8660a63969>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"weights_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'{:04d}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".hdf5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-3065adbd143b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(n_episodes):\n",
    "    state=env.reset()\n",
    "    state=np.reshape(state,[1,state_size])\n",
    "    batch_size=32\n",
    "    for time in range(500):\n",
    "        env.render()\n",
    "        action=agent.act(state) #action is going to be left or rigth (0 or 1)\n",
    "        next_state,reward,done,other_info=env.step(action)\n",
    "        \n",
    "        reward=reward if not done else -10 #randomly-->-10\n",
    "        next_state=np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(state,action,reward,next_state,done) #Experience for the agent\n",
    "        \n",
    "        if done:\n",
    "            print(\"Game Episode :{}/{} High Score :{} Exploration Rate:{:.2}\".format(e,20,t,agent.epsilon))\n",
    "            break\n",
    "        \n",
    "    if len(agent.memory)>batch_size:\n",
    "        agent.train(batch_size)\n",
    "    if e%50==0:\n",
    "        agent.save(output_dir+\"weights_\"+'{:04d}'.format(e)+\".hdf5\")\n",
    "\n",
    "    \n",
    "print(\"Deep Q-learner Model Trained\")\n",
    "env.close()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-10baf6094640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#this gives reward for the state given, there are 2 preddictions only left and right movement.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x=np.random.rand(1,4)\n",
    "model.predict(x) #this gives reward for the state given, there are 2 preddictions only left and right movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in range(10) if n%2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(n1,n2):\n",
    "    return n1+n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bbee93b405d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'my_func' is not defined"
     ]
    }
   ],
   "source": [
    "my_func(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
